#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ VLM ë¶„ì„ ë° íŒŒì¼ ì •ë¦¬ í†µí•© ì‹œìŠ¤í…œ
- ì „ì²´ ì´ë¯¸ì§€ ë¶„ì„ + í¬ë¡­ ì˜ì—­ ë¶„ì„ ìˆ˜í–‰
- JSON ê²°ê³¼ ì €ì¥ + íŒì •ë³„ íŒŒì¼ ìë™ ë¶„ë¦¬
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import json
import os
import re
import time
import shutil
from pathlib import Path
from typing import Dict, Any, List, Tuple
from collections import Counter

try:
    import torch
    from PIL import Image
    from transformers import LlavaForConditionalGeneration, AutoProcessor
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("pip install torch torchvision transformers pillow")
    DEPENDENCIES_AVAILABLE = False

# í”„ë¡¬í”„íŠ¸ë“¤
PROMPTS = [
    "USER: <image>\nCarefully examine this image. Do you see any birds, ducks, geese, or other waterfowl? "
    "Answer with JSON format: {\"label\":\"bird\",\"reason\":\"I can see a duck swimming\"} "
    "or {\"label\":\"background\",\"reason\":\"only water and vegetation\"}\nASSISTANT:",
    
    "USER: <image>\nLook closely at this image. Are there any living birds visible? "
    "This includes ducks, geese, swans, or any other waterfowl. "
    "Respond only in JSON: {\"label\":\"bird\",\"reason\":\"description of what you see\"} "
    "or {\"label\":\"background\",\"reason\":\"no birds visible\"}\nASSISTANT:",
    
    "USER: <image>\nIs there a bird in this image? "
    "Answer as JSON: {\"label\":\"bird\",\"reason\":\"bird description\"} "
    "or {\"label\":\"background\",\"reason\":\"no bird\"}\nASSISTANT:"
]

JSON_PAT = re.compile(r'\{[^}]*"label"[^}]*\}', re.S)

def load_model(model_name="llava-hf/llava-1.5-7b-hf", precision="fp16"):
    """ëª¨ë¸ ë¡œë“œ"""
    if not DEPENDENCIES_AVAILABLE:
        return None, None
    
    torch_dtype = {
        "bf16": torch.bfloat16, 
        "fp16": torch.float16, 
        "fp32": torch.float32
    }[precision]
    
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
    model = LlavaForConditionalGeneration.from_pretrained(
        model_name,
        torch_dtype=torch_dtype,
        low_cpu_mem_usage=True,
        device_map="auto",
    )
    
    processor = AutoProcessor.from_pretrained(model_name, use_fast=False)
    return model, processor

def run_vlm_inference(image: Image.Image, model, processor, use_multi_prompt=True) -> Dict[str, Any]:
    """VLM ì¶”ë¡  ì‹¤í–‰"""
    if not DEPENDENCIES_AVAILABLE or model is None:
        return {"label": "background", "reason": "dependencies-not-available", "raw": "", "prompt_id": -1}
    
    prompts = PROMPTS if use_multi_prompt else [PROMPTS[0]]
    results = []
    
    for i, prompt in enumerate(prompts):
        try:
            inputs = processor(text=prompt, images=image, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                out = model.generate(
                    **inputs,
                    do_sample=False,
                    temperature=0.0,
                    max_new_tokens=64,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
            
            text = processor.batch_decode(out, skip_special_tokens=True)[0]
            
            if "ASSISTANT:" in text:
                text = text.split("ASSISTANT:")[-1].strip()
            
            m = JSON_PAT.search(text)
            if m:
                try:
                    j = json.loads(m.group(0))
                    j["raw"] = text
                    j["prompt_id"] = i
                    results.append(j)
                except:
                    continue
                    
        except Exception:
            continue
    
    if not results:
        return {"label": "background", "reason": "no-valid-output", "raw": "parsing failed", "prompt_id": -1}
    
    # bird íŒì •ì´ ìˆìœ¼ë©´ ìš°ì„  ì„ íƒ
    bird_results = [r for r in results if r.get("label") == "bird"]
    return bird_results[0] if bird_results else results[0]

def load_yolo_labels(label_path: Path) -> List[Tuple[int, float, float, float, float]]:
    """YOLO ë¼ë²¨ íŒŒì¼ ë¡œë“œ"""
    boxes = []
    if not label_path.exists():
        return boxes
    
    with open(label_path, 'r') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split()
            if len(parts) >= 5:
                class_id = int(parts[0])
                center_x = float(parts[1])
                center_y = float(parts[2])
                width = float(parts[3])
                height = float(parts[4])
                boxes.append((class_id, center_x, center_y, width, height))
    return boxes

def yolo_to_bbox(center_x: float, center_y: float, width: float, height: float, 
                 img_width: int, img_height: int) -> Tuple[int, int, int, int]:
    """YOLO ì •ê·œí™” ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜"""
    x1 = int((center_x - width / 2) * img_width)
    y1 = int((center_y - height / 2) * img_height)
    x2 = int((center_x + width / 2) * img_width)
    y2 = int((center_y + height / 2) * img_height)
    
    x1 = max(0, min(x1, img_width))
    y1 = max(0, min(y1, img_height))
    x2 = max(0, min(x2, img_width))
    y2 = max(0, min(y2, img_height))
    
    return x1, y1, x2, y2

def crop_with_padding(image: Image.Image, bbox: Tuple[int, int, int, int], 
                     padding_ratio: float = 0.6) -> Image.Image:
    """ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ íŒ¨ë”©ê³¼ í•¨ê»˜ í¬ë¡­"""
    x1, y1, x2, y2 = bbox
    width = x2 - x1
    height = y2 - y1
    
    # ì‘ì€ ì˜ì—­ì€ ë” í° íŒ¨ë”©
    if width < 50 or height < 50:
        padding_ratio = max(padding_ratio, 1.0)
    
    pad_w = int(width * padding_ratio)
    pad_h = int(height * padding_ratio)
    
    x1_pad = max(0, x1 - pad_w)
    y1_pad = max(0, y1 - pad_h)
    x2_pad = min(image.width, x2 + pad_w)
    y2_pad = min(image.height, y2 + pad_h)
    
    return image.crop((x1_pad, y1_pad, x2_pad, y2_pad))

def analyze_image(img_path: Path, label_path: Path, model, processor, 
                 resize=336, padding_ratio=0.6):
    """ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰"""
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = Image.open(img_path).convert("RGB")
        img_width, img_height = img.size
        
        # YOLO ë¼ë²¨ ë¡œë“œ
        boxes = load_yolo_labels(label_path)
        
        # 1. ì „ì²´ ì´ë¯¸ì§€ ë¶„ì„
        full_img_resized = img.resize((resize, resize)) if resize > 0 else img
        full_result = run_vlm_inference(full_img_resized, model, processor, True)
        
        # 2. í¬ë¡­ ì˜ì—­ë“¤ ë¶„ì„
        crop_results = []
        for i, (class_id, center_x, center_y, width, height) in enumerate(boxes):
            bbox = yolo_to_bbox(center_x, center_y, width, height, img_width, img_height)
            cropped = crop_with_padding(img, bbox, padding_ratio)
            
            if resize > 0:
                cropped = cropped.resize((resize, resize))
            
            crop_result = run_vlm_inference(cropped, model, processor, True)
            crop_results.append({
                "box_id": i,
                "class_id": class_id,
                "bbox": list(bbox),
                "label": crop_result.get("label", "background"),
                "reason": crop_result.get("reason", ""),
                "crop_size": list(cropped.size)
            })
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ íŒì •
        bird_crops = [c for c in crop_results if c["label"] == "bird"]
        full_is_bird = full_result.get("label") == "bird"
        
        if len(bird_crops) > 0 and full_is_bird:
            hybrid_decision = "STRONG_BIRD"
            hybrid_reason = f"Both full image and {len(bird_crops)} crops detect birds"
        elif len(bird_crops) > 0:
            hybrid_decision = "CROP_BIRD"
            hybrid_reason = f"Only crops detect birds ({len(bird_crops)} found)"
        elif full_is_bird:
            hybrid_decision = "FULL_BIRD"
            hybrid_reason = "Only full image detects bird"
        else:
            hybrid_decision = "NO_BIRD"
            hybrid_reason = "No birds detected"
        
        # ê²°ê³¼ ë°˜í™˜
        return {
            "path": str(img_path),
            "full_analysis": full_result,
            "crop_analyses": crop_results,
            "hybrid_decision": hybrid_decision,
            "hybrid_reason": hybrid_reason,
            "num_detections": len(boxes),
            "num_bird_crops": len(bird_crops)
        }
        
    except Exception as e:
        print(f"ë¶„ì„ ì‹¤íŒ¨ ({img_path.name}): {e}")
        return None

def find_image_label_pairs(images_dir: Path, labels_dir: Path):
    """ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ìŒ ì°¾ê¸°"""
    pairs = []
    img_exts = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}
    
    for img_path in images_dir.rglob("*"):
        if img_path.suffix.lower() not in img_exts:
            continue
            
        rel_path = img_path.relative_to(images_dir)
        label_name = rel_path.stem + ".txt"
        label_path = labels_dir / rel_path.parent / label_name
        
        if label_path.exists():
            pairs.append((img_path, label_path))
    
    return pairs

def organize_files_by_decision(input_dir: Path, results: List[Dict], organize_files: bool = True):
    """
    ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ íŒŒì¼ì„ íŒì •ë³„ í´ë”ë¡œ ì •ë¦¬
    
    Args:
        input_dir: ì…ë ¥ í´ë” ê²½ë¡œ
        results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        organize_files: íŒŒì¼ ì •ë¦¬ ìˆ˜í–‰ ì—¬ë¶€
    """
    if not organize_files:
        print("ğŸ“Œ íŒŒì¼ ì •ë¦¬ ê±´ë„ˆëœ€ (--no-organize ì˜µì…˜)")
        return
    
    print("\nğŸ“‚ íŒŒì¼ ì •ë¦¬ ì‹œì‘...")
    
    # í´ë” í™•ì¸
    original_frames_dir = input_dir / "original_frames"
    detected_frames_dir = input_dir / "detected_frames"
    labels_dir = input_dir / "labels"
    
    has_detected_frames = detected_frames_dir.exists()
    
    # íŒì •ë³„ ì¹´ìš´íŠ¸
    decision_counts = {}
    for result in results:
        decision = result['hybrid_decision']
        decision_counts[decision] = decision_counts.get(decision, 0) + 1
    
    # íŒì •ë³„ í´ë” ìƒì„±
    for decision in decision_counts.keys():
        # original_frames
        (original_frames_dir / decision).mkdir(exist_ok=True)
        
        # detected_frames (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
        if has_detected_frames:
            (detected_frames_dir / decision).mkdir(exist_ok=True)
        
        # labels
        (labels_dir / decision).mkdir(exist_ok=True)
    
    # íŒŒì¼ ì´ë™
    moved_count = 0
    detected_moved_count = 0
    label_moved_count = 0
    
    for result in results:
        img_path = Path(result['path'])
        decision = result['hybrid_decision']
        
        if not img_path.exists():
            continue
        
        try:
            # 1. original_frames ì´ë™
            target_path = original_frames_dir / decision / img_path.name
            shutil.move(str(img_path), str(target_path))
            moved_count += 1
            
            # 2. detected_frames ì´ë™ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
            if has_detected_frames:
                detected_path = detected_frames_dir / img_path.name
                if detected_path.exists():
                    detected_target = detected_frames_dir / decision / img_path.name
                    shutil.move(str(detected_path), str(detected_target))
                    detected_moved_count += 1
            
            # 3. labels ì´ë™
            label_filename = img_path.stem + ".txt"
            label_path = labels_dir / label_filename
            if label_path.exists():
                label_target = labels_dir / decision / label_filename
                shutil.move(str(label_path), str(label_target))
                label_moved_count += 1
                
        except Exception as e:
            print(f"  âš ï¸ íŒŒì¼ ì´ë™ ì‹¤íŒ¨ ({img_path.name}): {e}")
    
    # ì •ë¦¬ ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š original_frames ì´ë™: {moved_count}ê°œ")
    if has_detected_frames:
        print(f"ğŸ“Š detected_frames ì´ë™: {detected_moved_count}ê°œ")
    print(f"ğŸ“Š labels ì´ë™: {label_moved_count}ê°œ")
    
    print(f"\nğŸ“ íŒì •ë³„ íŒŒì¼ ë¶„í¬:")
    for decision, count in decision_counts.items():
        print(f"  {decision}: {count}ê°œ")

def main(input_dir, model="llava-hf/llava-1.5-7b-hf", precision="fp16", 
         resize=336, padding_ratio=0.6, limit=0, organize_files=True):
    """í•˜ì´ë¸Œë¦¬ë“œ VLM ë¶„ì„ ë° íŒŒì¼ ì •ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # ì…ë ¥ í´ë” í™•ì¸
    input_dir = Path(input_dir)
    if not input_dir.exists():
        print(f"âŒ ì…ë ¥ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_dir}")
        return
    
    print(f"âœ… ì…ë ¥ í´ë”: {input_dir}")
    
    # original_framesì™€ labels í´ë” ì°¾ê¸°
    images_dir = input_dir / "original_frames"
    labels_dir = input_dir / "labels"
    
    # í´ë” ì¡´ì¬ í™•ì¸
    if not images_dir.exists():
        print(f"âŒ original_frames í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {images_dir}")
        return
    
    if not labels_dir.exists():
        print(f"âŒ labels í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {labels_dir}")
        return
    
    print(f"   - ì´ë¯¸ì§€: {images_dir}")
    print(f"   - ë¼ë²¨: {labels_dir}")
    
    # detected_frames í´ë” í™•ì¸
    detected_frames_dir = input_dir / "detected_frames"
    if detected_frames_dir.exists():
        print(f"   - ê²€ì¶œ ì´ë¯¸ì§€: {detected_frames_dir}")
    
    # JSON íŒŒì¼ ê²½ë¡œ - ì…ë ¥ í´ë”ì— ì €ì¥
    results_filename = "hybrid_results.json"
    results_file = input_dir / results_filename
    
    # ê¸°ì¡´ ê²°ê³¼ í™•ì¸ ë° ì§„í–‰ ìƒí™© ë¶„ì„
    existing_results = []
    processed_paths = set()
    
    if results_file.exists():
        print(f"\nğŸ“„ ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ë°œê²¬: {results_file}")
        print("=" * 60)
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                existing_results = json.load(f)
            
            # ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            processed_paths = {r['path'] for r in existing_results}
            
            # ì§„í–‰ ìƒí™© ë¶„ì„
            if existing_results:
                print(f"ğŸ“Š ì´ì „ ì‘ì—… ì§„í–‰ ìƒí™©:")
                print(f"   - ì²˜ë¦¬ ì™„ë£Œ: {len(existing_results)}ê°œ ì´ë¯¸ì§€")
                
                # íŒì •ë³„ í†µê³„
                decision_stats = Counter([r['hybrid_decision'] for r in existing_results])
                print(f"\n   íŒì • ë¶„í¬:")
                for decision, count in sorted(decision_stats.items()):
                    print(f"     â€¢ {decision}: {count}ê°œ")
                
                # ë§ˆì§€ë§‰ ì²˜ë¦¬ íŒŒì¼
                last_result = existing_results[-1]
                last_path = Path(last_result['path'])
                print(f"\n   ë§ˆì§€ë§‰ ì²˜ë¦¬ íŒŒì¼: {last_path.name}")
                print(f"   ë§ˆì§€ë§‰ íŒì •: {last_result['hybrid_decision']}")
                
                # ì‚¬ìš©ì í™•ì¸
                print("\nğŸ”„ ì´ì–´ì„œ ì‘ì—…ì„ ì§„í–‰í•©ë‹ˆë‹¤...")
                print("=" * 60)
                
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON íŒŒì¼ ì†ìƒ ê°ì§€: {e}")
            backup_file = results_file.with_suffix('.backup.json')
            shutil.copy2(results_file, backup_file)
            print(f"   ë°±ì—… ìƒì„±: {backup_file}")
            
            user_input = input("ìƒˆë¡œ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if user_input.lower() != 'y':
                print("ì‘ì—… ì·¨ì†Œ")
                return
            existing_results = []
            processed_paths = set()
    else:
        print(f"\nğŸ“„ ìƒˆ ì‘ì—… ì‹œì‘")
        print(f"   ê²°ê³¼ íŒŒì¼ ìƒì„±: {results_file}")
        # ë¹ˆ JSON íŒŒì¼ ìƒì„±
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump([], f)
        print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model_obj, processor = load_model(model, precision)
    if model_obj is None:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # ì´ë¯¸ì§€-ë¼ë²¨ ìŒ ì°¾ê¸°
    all_pairs = find_image_label_pairs(images_dir, labels_dir)
    
    if not all_pairs:
        print(f"âš ï¸ ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€-ë¼ë²¨ ìŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì²˜ë¦¬í•  íŒŒì¼ í•„í„°ë§
    pairs_to_process = []
    skipped_count = 0
    
    print(f"\nğŸ“‚ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
    for img_path, label_path in all_pairs:
        if str(img_path) not in processed_paths:
            pairs_to_process.append((img_path, label_path))
        else:
            skipped_count += 1
    
    # ìƒíƒœ ìš”ì•½
    print(f"\nğŸ“Š ì‘ì—… ìƒíƒœ:")
    print(f"   ì „ì²´ íŒŒì¼: {len(all_pairs)}ê°œ")
    print(f"   âœ… ì´ë¯¸ ì²˜ë¦¬ë¨: {skipped_count}ê°œ")
    print(f"   â³ ì²˜ë¦¬ ëŒ€ê¸°: {len(pairs_to_process)}ê°œ")
    
    if not pairs_to_process:
        print(f"\nâœ¨ ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        if existing_results:
            print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
            decisions = [r['hybrid_decision'] for r in existing_results]
            decision_count = Counter(decisions)
            
            for decision, count in sorted(decision_count.items()):
                percentage = count/len(existing_results)*100
                print(f"   {decision}: {count}ê°œ ({percentage:.1f}%)")
            
            # âœ¨ íŒŒì¼ ì •ë¦¬ ì˜µì…˜ ì¶”ê°€
            if organize_files:
                print(f"\nğŸ“‚ ê¸°ì¡´ ê²°ê³¼ë¡œ íŒŒì¼ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
                if input().lower() == 'y':
                    organize_files_by_decision(input_dir, existing_results, True)
        return
    
    # limit ì ìš©
    if limit > 0 and limit < len(pairs_to_process):
        pairs_to_process = pairs_to_process[:limit]
        print(f"   âš ï¸ limit ì ìš©: {limit}ê°œë§Œ ì²˜ë¦¬")
    
    # ì§„í–‰ë¥  í‘œì‹œ ì¤€ë¹„
    total_to_process = len(pairs_to_process)
    already_processed = len(existing_results)
    grand_total = len(all_pairs)
    
    print(f"\nğŸš€ ë¶„ì„ ì‹œì‘!")
    print("=" * 60)
    
    # ë¶„ì„ ìˆ˜í–‰
    success_count = 0
    failed_count = 0
    start_time = time.time()
    
    for i, (img_path, label_path) in enumerate(pairs_to_process):
        # ì§„í–‰ë¥  ê³„ì‚°
        current_progress = already_processed + i + 1
        percentage = (current_progress / grand_total) * 100
        
        print(f"\n[ì „ì²´ {current_progress}/{grand_total} ({percentage:.1f}%)] "
              f"[í˜„ì¬ ì„¸ì…˜ {i+1}/{total_to_process}]")
        print(f"ì²˜ë¦¬ ì¤‘: {img_path.name}")
        
        try:
            # ì´ë¯¸ì§€ ë¶„ì„ ì‹¤í–‰
            result = analyze_image(
                img_path, label_path, model_obj, processor,
                resize, padding_ratio
            )
            
            if result:
                # ê¸°ì¡´ ê²°ê³¼ ì½ê¸°
                with open(results_file, 'r', encoding='utf-8') as f:
                    current_results = json.load(f)
                
                # ìƒˆ ê²°ê³¼ ì¶”ê°€
                current_results.append(result)
                
                # ì•ˆì „í•˜ê²Œ ì €ì¥ (ì„ì‹œ íŒŒì¼ ì‚¬ìš©)
                temp_file = results_file.with_suffix('.tmp')
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(current_results, f, ensure_ascii=False, indent=2)
                temp_file.replace(results_file)
                
                success_count += 1
                print(f"  âœ… íŒì •: {result['hybrid_decision']}")
                print(f"  ğŸ“Š í¬ë¡­ ë¶„ì„: {result['num_bird_crops']}/{result['num_detections']} ìƒˆ ê²€ì¶œ")
                
                # ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ ê³„ì‚° (10ê°œ ì²˜ë¦¬ í›„ë¶€í„°)
                if i >= 9:
                    elapsed = time.time() - start_time
                    avg_time = elapsed / (i + 1)
                    remaining = (total_to_process - i - 1) * avg_time
                    
                    hours = int(remaining // 3600)
                    minutes = int((remaining % 3600) // 60)
                    seconds = int(remaining % 60)
                    
                    if hours > 0:
                        print(f"  â±ï¸ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {hours}ì‹œê°„ {minutes}ë¶„")
                    elif minutes > 0:
                        print(f"  â±ï¸ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ")
                    else:
                        print(f"  â±ï¸ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {seconds}ì´ˆ")
                        
            else:
                failed_count += 1
                print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨")
                
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            print(f"ğŸ’¾ {success_count}ê°œ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì‘ì—…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return
            
        except Exception as e:
            failed_count += 1
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    # ì‘ì—… ì™„ë£Œ
    elapsed_total = time.time() - start_time
    print("\n" + "=" * 60)
    print(f"âœ¨ í˜„ì¬ ì„¸ì…˜ ì‘ì—… ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ ì‹œê°„: {elapsed_total/60:.1f}ë¶„")
    print(f"   ì„±ê³µ: {success_count}ê°œ")
    print(f"   ì‹¤íŒ¨: {failed_count}ê°œ")
    
    # ìµœì¢… ê²°ê³¼ ë¡œë“œ
    with open(results_file, 'r', encoding='utf-8') as f:
        final_results = json.load(f)
    
    print(f"\nğŸ“Š ì „ì²´ ëˆ„ì  ê²°ê³¼:")
    print(f"   ì´ ì²˜ë¦¬ëœ íŒŒì¼: {len(final_results)}/{len(all_pairs)}ê°œ")
    
    # ìµœì¢… íŒì • ë¶„í¬
    if final_results:
        decisions = [r['hybrid_decision'] for r in final_results]
        decision_count = Counter(decisions)
        
        print(f"\nğŸ¯ ì „ì²´ íŒì • ë¶„í¬:")
        for decision, count in sorted(decision_count.items()):
            percentage = count/len(final_results)*100
            print(f"   {decision}: {count}ê°œ ({percentage:.1f}%)")
    
    # íŒŒì¼ ì •ë¦¬ ìˆ˜í–‰
    if organize_files and success_count > 0:
        print(f"\nğŸ“‚ íŒŒì¼ ì •ë¦¬ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
        if input().lower() == 'y':
            # ì´ë²ˆì— ì²˜ë¦¬ëœ ê²°ê³¼ë§Œ ì •ë¦¬
            new_results = final_results[-success_count:]
            organize_files_by_decision(input_dir, new_results, True)
    
    print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='í•˜ì´ë¸Œë¦¬ë“œ VLM ë¶„ì„ ë° íŒŒì¼ ì •ë¦¬')
    parser.add_argument('input_dir', type=str, help='ì…ë ¥ í´ë” ê²½ë¡œ')
    parser.add_argument('--model', type=str, default="llava-hf/llava-1.5-7b-hf", 
                       help='VLM ëª¨ë¸ (ê¸°ë³¸: llava-hf/llava-1.5-7b-hf)')
    parser.add_argument('--precision', type=str, default="fp16", 
                       choices=["bf16", "fp16", "fp32"],
                       help='ëª¨ë¸ ì •ë°€ë„ (ê¸°ë³¸: fp16)')
    parser.add_argument('--resize', type=int, default=336, 
                       help='VLM ì…ë ¥ í¬ê¸° (ê¸°ë³¸: 336)')
    parser.add_argument('--padding', type=float, default=0.6, 
                       help='í¬ë¡­ íŒ¨ë”© ë¹„ìœ¨ (ê¸°ë³¸: 0.6)')
    parser.add_argument('--limit', type=int, default=0, 
                       help='ì²˜ë¦¬í•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜, 0=ì „ì²´ (ê¸°ë³¸: 0)')
    parser.add_argument('--no-organize', action='store_true', 
                       help='íŒŒì¼ ì •ë¦¬ ê±´ë„ˆë›°ê¸° (JSONë§Œ ìƒì„±)')
    
    args = parser.parse_args()
    
    # ì‹¤í–‰
    main(
        input_dir=args.input_dir,
        model=args.model,
        precision=args.precision,
        resize=args.resize,
        padding_ratio=args.padding,
        limit=args.limit,
        organize_files=not args.no_organize
    )
